# ğŸ“Š SaaS Revenue Analytics

Proyecto de anÃ¡lisis de ingresos y cancelaciones en una empresa SaaS utilizando **PySpark**, **Python**, **Power BI** y un enfoque orientado a datos.

Incluye limpieza de datos, construcciÃ³n de un modelo dimensional (esquema estrella), integraciÃ³n con PySpark y visualizaciÃ³n futura con herramientas BI.

---

## ğŸ¯ Objetivo

- Analizar los ingresos mensuales generados por usuarios.
- Identificar tasas de cancelaciÃ³n (churn) y razones mÃ¡s frecuentes.
- Detectar patrones por paÃ­s, tipo de suscripciÃ³n y duraciÃ³n del plan.
- Implementar un flujo ETL escalable con PySpark.
- Construir una **tabla de hechos** para anÃ¡lisis avanzado en Power BI.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Python** (pandas, matplotlib)
- **PySpark** (procesamiento distribuido)
- **Power BI** (visualizaciÃ³n futura)
- **SQL** (consultas exploratorias)
- **Jupyter Notebooks**
- **Git & GitHub**

---

## ğŸ§± Modelo Dimensional

Se implementÃ³ un esquema tipo **estrella** compuesto por:

### ğŸ“Œ Tabla de Hechos â€“ `fact_revenue`

| Campo         | DescripciÃ³n                          |
|---------------|--------------------------------------|
| UserID        | Usuario que realiza el pago          |
| PaymentDate   | Fecha del pago                       |
| Amount        | Monto pagado en USD                  |
| PaymentMethod | MÃ©todo de pago                       |
| Month         | Mes del pago (YYYY-MM)               |

---

### ğŸ“‚ Tablas de DimensiÃ³n

#### `dim_user`
- `UserID`  
- `Country`  
- `RegistrationDate`  

#### `dim_subscription`
- `UserID`  
- `Plan` (Basic, Standard, Premium)  
- `MonthlyPrice`  
- `DurationMonths`  
- `StartDate`  

#### `dim_churn`
- `UserID`  
- `ChurnDate`  
- `ChurnReason`  

> *Posible extensiÃ³n futura: `dim_date` para anÃ¡lisis temporal mÃ¡s detallado.*

---

## ğŸ“ Estructura del proyecto

saas-revenue-analytics/
â”‚
â”œâ”€â”€ data/ # Datos originales y limpios (CSV)
â”œâ”€â”€ notebooks/ # Limpieza y anÃ¡lisis exploratorio
â”œâ”€â”€ spark_jobs/ # Scripts PySpark para construir tabla de hechos
â”œâ”€â”€ sql/ # Consultas y transformaciones SQL
â”œâ”€â”€ dashboards/ # Power BI (prÃ³ximamente)
â”œâ”€â”€ dags/ # AutomatizaciÃ³n futura con Airflow
â”œâ”€â”€ README.md # DocumentaciÃ³n principal del proyecto
â”œâ”€â”€ requirements.txt # LibrerÃ­as necesarias
â””â”€â”€ .gitignore


---

## âš™ï¸ CÃ³mo reproducir el proyecto

1. Clona el repositorio
2. Crea y activa un entorno virtual
3. Instala dependencias: `pip install -r requirements.txt`
4. Ejecuta los notebooks para explorar y limpiar los datos (notebooks/exploracion_inicial.ipynb)
5. Ejecuta la construcciÃ³n de la tabla de hechos con Spark (python spark_jobs/build_fact_table.py)
6. Carga el resultado en Power BI para anÃ¡lisis interactivo


## ğŸ“ˆ Dashboard 

Dashboard interactivo construido en Power BI con indicadores clave.

![Vista general del dashboard]()

## ğŸ§  Observaciones clave del anÃ¡lisis

- 

## ğŸ‘¤ Autor

- [Alan Arturo Cano Sanchez](https://www.linkedin.com/in/alan-arturo-cano-sanchez-511855361)
- Egresado de IngenierÃ­a en Datos e Inteligencia Organizacional
